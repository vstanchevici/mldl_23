{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class LinRegAlgebra():\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.theta)\n",
    "    \n",
    "\n",
    "class RegOptimizer():\n",
    "    def __init__(self, alpha, n_iters):\n",
    "        self.theta = None\n",
    "        self._alpha = alpha\n",
    "        self._n_iters = n_iters\n",
    "    \n",
    "    def gradient_step(self, theta, theta_grad):\n",
    "        return theta - self._alpha * theta_grad\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def optimize(self, X, y, start_theta, n_iters):\n",
    "        theta = start_theta.copy()\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            theta_grad = self.grad_func(X, y, theta)\n",
    "            #checking max value\n",
    "            max_val = max(abs(theta_grad.max()), abs(theta_grad.min()))\n",
    "            if (max_val < 0.01):\n",
    "                print(\"max_val =\", max_val, \", iter =\", i)\n",
    "                break\n",
    "            theta = self.gradient_step(theta, theta_grad)\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        start_theta = np.ones(m)\n",
    "        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class LinReg(RegOptimizer):\n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise Exception('You should train the model first')\n",
    "        \n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def print_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n",
    "    \n",
    "def prepare_boston_data():\n",
    "    raw_df = pd.read_csv('../../data/boston.csv', sep=\"\\s+\", skiprows=22, header=None)\n",
    "    X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "    y = raw_df.values[1::2, 2]\n",
    "    # Нормализовать даннные с помощью стандартной нормализации\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def prepare_boston_data_new():\n",
    "    raw_df = pd.read_csv('../../data/boston.csv', sep=\"\\s+\", skiprows=22, header=None)\n",
    "    X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "    y = raw_df.values[1::2, 2]\n",
    "    \n",
    "    X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])\n",
    "    \n",
    "    # Нормализовать даннные с помощью стандартной нормализации\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_validate(X, y):\n",
    "    # Разбить данные на train/valid\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)\n",
    "\n",
    "    # Создать и обучить линейную регрессию\n",
    "    linreg_alg = LinRegAlgebra()\n",
    "    linreg_alg.fit(X_train, y_train)\n",
    "\n",
    "    # Сделать предсказания по валидционной выборке\n",
    "    y_pred = linreg_alg.predict(X_valid)\n",
    "\n",
    "    # Посчитать значение ошибок MSE и RMSE для валидационных данных\n",
    "    print_regression_metrics(y_valid, y_pred)\n",
    "\n",
    "\n",
    "#X, y = prepare_boston_data_new()\n",
    "#train_validate(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_val = 0.009929090072607566 , iter = 212\n"
     ]
    }
   ],
   "source": [
    "#3.7.1 problem\n",
    "X, y = prepare_boston_data()\n",
    "linreg_alg = LinReg(0.2, 10000)\n",
    "linreg_alg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 13.59, RMSE = 3.69\n"
     ]
    }
   ],
   "source": [
    "#3.7.2 problem\n",
    "def prepare_boston_data2():\n",
    "    raw_df = pd.read_csv('../../data/boston.csv', sep=\"\\s+\", skiprows=22, header=None)\n",
    "    X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "    y = raw_df.values[1::2, 2]\n",
    "    \n",
    "    X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3, X[:, 7:8] ** 2,])\n",
    "    \n",
    "    # Нормализовать даннные с помощью стандартной нормализации\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_boston_data2()\n",
    "train_validate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 14.28, RMSE = 3.78\n"
     ]
    }
   ],
   "source": [
    "#3.7.3 problem\n",
    "def prepare_boston_data3():\n",
    "    raw_df = pd.read_csv('../../data/boston.csv', sep=\"\\s+\", skiprows=22, header=None)\n",
    "    X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "    y = raw_df.values[1::2, 2]\n",
    "    \n",
    "    X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])\n",
    "    \n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_boston_data3()\n",
    "train_validate(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " В принципе всё было понятно, но некоторые вещи были двусмысленные/недостаточно уточнены (пример 3.7.1 я использовал train_validate но надо было давать алгоритму X и y какие есть без train_test_split), по краинеи мере для меня."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
